<html>
<head>
<title>device_init.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #5f826b; font-style: italic;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
device_init.py</font>
</center></td></tr></table>
<pre><span class="s0"># Re export</span>
<span class="s2">import </span><span class="s1">sys</span>
<span class="s2">from </span><span class="s1">numba</span><span class="s3">.</span><span class="s1">cuda </span><span class="s2">import </span><span class="s1">cg</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">stubs </span><span class="s2">import </span><span class="s3">(</span><span class="s1">threadIdx</span><span class="s3">, </span><span class="s1">blockIdx</span><span class="s3">, </span><span class="s1">blockDim</span><span class="s3">, </span><span class="s1">gridDim</span><span class="s3">, </span><span class="s1">laneid</span><span class="s3">, </span><span class="s1">warpsize</span><span class="s3">,</span>
                    <span class="s1">syncwarp</span><span class="s3">, </span><span class="s1">shared</span><span class="s3">, </span><span class="s1">local</span><span class="s3">, </span><span class="s1">const</span><span class="s3">, </span><span class="s1">atomic</span><span class="s3">,</span>
                    <span class="s1">shfl_sync_intrinsic</span><span class="s3">, </span><span class="s1">vote_sync_intrinsic</span><span class="s3">, </span><span class="s1">match_any_sync</span><span class="s3">,</span>
                    <span class="s1">match_all_sync</span><span class="s3">, </span><span class="s1">threadfence_block</span><span class="s3">, </span><span class="s1">threadfence_system</span><span class="s3">,</span>
                    <span class="s1">threadfence</span><span class="s3">, </span><span class="s1">selp</span><span class="s3">, </span><span class="s1">popc</span><span class="s3">, </span><span class="s1">brev</span><span class="s3">, </span><span class="s1">clz</span><span class="s3">, </span><span class="s1">ffs</span><span class="s3">, </span><span class="s1">fma</span><span class="s3">, </span><span class="s1">cbrt</span><span class="s3">,</span>
                    <span class="s1">activemask</span><span class="s3">, </span><span class="s1">lanemask_lt</span><span class="s3">, </span><span class="s1">nanosleep</span><span class="s3">, </span><span class="s1">fp16</span><span class="s3">,</span>
                    <span class="s1">_vector_type_stubs</span><span class="s3">)</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">intrinsics </span><span class="s2">import </span><span class="s3">(</span><span class="s1">grid</span><span class="s3">, </span><span class="s1">gridsize</span><span class="s3">, </span><span class="s1">syncthreads</span><span class="s3">, </span><span class="s1">syncthreads_and</span><span class="s3">,</span>
                         <span class="s1">syncthreads_count</span><span class="s3">, </span><span class="s1">syncthreads_or</span><span class="s3">)</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">cudadrv</span><span class="s3">.</span><span class="s1">error </span><span class="s2">import </span><span class="s1">CudaSupportError</span>
<span class="s2">from </span><span class="s1">numba</span><span class="s3">.</span><span class="s1">cuda</span><span class="s3">.</span><span class="s1">cudadrv</span><span class="s3">.</span><span class="s1">driver </span><span class="s2">import </span><span class="s3">(</span><span class="s1">BaseCUDAMemoryManager</span><span class="s3">,</span>
                                       <span class="s1">HostOnlyCUDAMemoryManager</span><span class="s3">,</span>
                                       <span class="s1">GetIpcHandleMixin</span><span class="s3">, </span><span class="s1">MemoryPointer</span><span class="s3">,</span>
                                       <span class="s1">MappedMemory</span><span class="s3">, </span><span class="s1">PinnedMemory</span><span class="s3">, </span><span class="s1">MemoryInfo</span><span class="s3">,</span>
                                       <span class="s1">IpcHandle</span><span class="s3">, </span><span class="s1">set_memory_manager</span><span class="s3">)</span>
<span class="s2">from </span><span class="s1">numba</span><span class="s3">.</span><span class="s1">cuda</span><span class="s3">.</span><span class="s1">cudadrv</span><span class="s3">.</span><span class="s1">runtime </span><span class="s2">import </span><span class="s1">runtime</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">cudadrv </span><span class="s2">import </span><span class="s1">nvvm</span>
<span class="s2">from </span><span class="s1">numba</span><span class="s3">.</span><span class="s1">cuda </span><span class="s2">import </span><span class="s1">initialize</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">errors </span><span class="s2">import </span><span class="s1">KernelRuntimeError</span>

<span class="s2">from </span><span class="s3">.</span><span class="s1">decorators </span><span class="s2">import </span><span class="s1">jit</span><span class="s3">, </span><span class="s1">declare_device</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">api </span><span class="s2">import </span><span class="s3">*</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">api </span><span class="s2">import </span><span class="s1">_auto_device</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">args </span><span class="s2">import </span><span class="s1">In</span><span class="s3">, </span><span class="s1">Out</span><span class="s3">, </span><span class="s1">InOut</span>

<span class="s2">from </span><span class="s3">.</span><span class="s1">intrinsic_wrapper </span><span class="s2">import </span><span class="s3">(</span><span class="s1">all_sync</span><span class="s3">, </span><span class="s1">any_sync</span><span class="s3">, </span><span class="s1">eq_sync</span><span class="s3">, </span><span class="s1">ballot_sync</span><span class="s3">,</span>
                                <span class="s1">shfl_sync</span><span class="s3">, </span><span class="s1">shfl_up_sync</span><span class="s3">, </span><span class="s1">shfl_down_sync</span><span class="s3">,</span>
                                <span class="s1">shfl_xor_sync</span><span class="s3">)</span>

<span class="s2">from </span><span class="s3">.</span><span class="s1">kernels </span><span class="s2">import </span><span class="s1">reduction</span>

<span class="s1">reduce </span><span class="s3">= </span><span class="s1">Reduce </span><span class="s3">= </span><span class="s1">reduction</span><span class="s3">.</span><span class="s1">Reduce</span>

<span class="s0"># Expose vector type constructors and aliases as module level attributes.</span>
<span class="s2">for </span><span class="s1">vector_type_stub </span><span class="s2">in </span><span class="s1">_vector_type_stubs</span><span class="s3">:</span>
    <span class="s1">setattr</span><span class="s3">(</span><span class="s1">sys</span><span class="s3">.</span><span class="s1">modules</span><span class="s3">[</span><span class="s1">__name__</span><span class="s3">], </span><span class="s1">vector_type_stub</span><span class="s3">.</span><span class="s1">__name__</span><span class="s3">, </span><span class="s1">vector_type_stub</span><span class="s3">)</span>
    <span class="s2">for </span><span class="s1">alias </span><span class="s2">in </span><span class="s1">vector_type_stub</span><span class="s3">.</span><span class="s1">aliases</span><span class="s3">:</span>
        <span class="s1">setattr</span><span class="s3">(</span><span class="s1">sys</span><span class="s3">.</span><span class="s1">modules</span><span class="s3">[</span><span class="s1">__name__</span><span class="s3">], </span><span class="s1">alias</span><span class="s3">, </span><span class="s1">vector_type_stub</span><span class="s3">)</span>
<span class="s2">del </span><span class="s1">vector_type_stub</span><span class="s3">, </span><span class="s1">_vector_type_stubs</span>


<span class="s2">def </span><span class="s1">is_available</span><span class="s3">():</span>
    <span class="s4">&quot;&quot;&quot;Returns a boolean to indicate the availability of a CUDA GPU. 
 
    This will initialize the driver if it hasn't been initialized. 
    &quot;&quot;&quot;</span>
    <span class="s0"># whilst `driver.is_available` will init the driver itself,</span>
    <span class="s0"># the driver initialization may raise and as a result break</span>
    <span class="s0"># test discovery/orchestration as `cuda.is_available` is often</span>
    <span class="s0"># used as a guard for whether to run a CUDA test, the try/except</span>
    <span class="s0"># below is to handle this case.</span>
    <span class="s1">driver_is_available </span><span class="s3">= </span><span class="s2">False</span>
    <span class="s2">try</span><span class="s3">:</span>
        <span class="s1">driver_is_available </span><span class="s3">= </span><span class="s1">driver</span><span class="s3">.</span><span class="s1">driver</span><span class="s3">.</span><span class="s1">is_available</span>
    <span class="s2">except </span><span class="s1">CudaSupportError</span><span class="s3">:</span>
        <span class="s2">pass</span>

    <span class="s2">return </span><span class="s1">driver_is_available </span><span class="s2">and </span><span class="s1">nvvm</span><span class="s3">.</span><span class="s1">is_available</span><span class="s3">()</span>


<span class="s2">def </span><span class="s1">is_supported_version</span><span class="s3">():</span>
    <span class="s4">&quot;&quot;&quot;Returns True if the CUDA Runtime is a supported version. 
 
    Unsupported versions (e.g. newer versions than those known to Numba) 
    may still work; this function provides a facility to check whether the 
    current Numba version is tested and known to work with the current 
    runtime version. If the current version is unsupported, the caller can 
    decide how to act. Options include: 
 
    - Continuing silently, 
    - Emitting a warning, 
    - Generating an error or otherwise preventing the use of CUDA. 
    &quot;&quot;&quot;</span>

    <span class="s2">return </span><span class="s1">runtime</span><span class="s3">.</span><span class="s1">is_supported_version</span><span class="s3">()</span>


<span class="s2">def </span><span class="s1">cuda_error</span><span class="s3">():</span>
    <span class="s4">&quot;&quot;&quot;Returns None if there was no error initializing the CUDA driver. 
    If there was an error initializing the driver, a string describing the 
    error is returned. 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">driver</span><span class="s3">.</span><span class="s1">driver</span><span class="s3">.</span><span class="s1">initialization_error</span>


<span class="s1">initialize</span><span class="s3">.</span><span class="s1">initialize_all</span><span class="s3">()</span>
</pre>
</body>
</html>